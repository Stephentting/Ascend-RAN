#!/usr/bin/python3
# coding=utf-8
import torch
import torch_npu
import numpy as np
from torch_npu.testing.testcase import TestCase, run_tests
import sys
import os
import time

# å°†å½“å‰ç›®å½•åŠ å…¥è·¯å¾„ä»¥ä¾¿å¯¼å…¥ç¼–è¯‘å¥½çš„ pybind æ¨¡å—
sys.path.append(os.getcwd())
try:
    import matmul_LS_custom
except ImportError:
    print("é”™è¯¯: æ‰¾ä¸åˆ° matmul_LS_custom æ¨¡å—ï¼Œè¯·ç¡®ä¿å·²å®Œæˆç¼–è¯‘å¹¶åœ¨å½“å‰ç›®å½•ä¸‹ã€‚")
    sys.exit(1)

class TestLSEstimatorCustom(TestCase):
    def test_ls_estimator_performance_and_precision(self):
        # 1. å‚æ•°é…ç½®
        BATCH_SIZE = 1192
        K_DIM = 32
        N_DIM = 512
        WARMUP_ITERS = 20    # é¢„çƒ­æ¬¡æ•°
        TEST_ITERS = 100     # æ­£å¼ç»Ÿè®¡è€—æ—¶æ¬¡æ•°

        # 2. è¯»å–æ•°æ®
        x1_path = "../input/x1_gm.bin"
        x2_path = "../input/x2_gm.bin"
        golden_path = "../output/golden.bin"

        if not (os.path.exists(x1_path) and os.path.exists(x2_path) and os.path.exists(golden_path)):
            print("é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ•°æ®æ–‡ä»¶ã€‚")
            return

        x1_np = np.fromfile(x1_path, dtype=np.float16).reshape(BATCH_SIZE, K_DIM)
        x2_np = np.fromfile(x2_path, dtype=np.float16).reshape(K_DIM, N_DIM)
        golden_np = np.fromfile(golden_path, dtype=np.float32).reshape(BATCH_SIZE, N_DIM)

        # æ¬è¿åˆ° NPU
        a = torch.from_numpy(x1_np).npu()
        b = torch.from_numpy(x2_np).npu()
        golden = torch.from_numpy(golden_np).npu()

        print(f"--- å¼€å§‹æ€§èƒ½ä¸ç²¾åº¦æµ‹è¯• (Batch={BATCH_SIZE}) ---")

        # 3. é¢„çƒ­ (Warmup)
        # ç›®çš„ï¼šåˆå§‹åŒ–ä¸Šä¸‹æ–‡ã€åŠ è½½ç®—å­äºŒè¿›åˆ¶æ–‡ä»¶åˆ°æŒ‡ä»¤ç¼“å­˜
        print(f"æ­£åœ¨é¢„çƒ­ {WARMUP_ITERS} æ¬¡...")
        for _ in range(WARMUP_ITERS):
            _ = matmul_LS_custom.run_ls_estimator(a, b)
        torch.npu.synchronize() # ç­‰å¾…é¢„çƒ­ä»»åŠ¡åœ¨ NPU ä¸Šå…¨éƒ¨å®Œæˆ

        # 4. æ€§èƒ½æµ‹è¯• (Timing)
        print(f"æ­£åœ¨è¿è¡Œæ€§èƒ½æµ‹è¯• {TEST_ITERS} æ¬¡...")
        start_event = torch.npu.Event(enable_timing=True)
        end_event = torch.npu.Event(enable_timing=True)

        start_event.record()
        for _ in range(TEST_ITERS):
            output = matmul_LS_custom.run_ls_estimator(a, b)
        end_event.record()
        
        torch.npu.synchronize() # ç¡®ä¿æ‰€æœ‰è®¡ç®—å®Œæˆ
        # è®¡ç®—æ¯«ç§’è€—æ—¶å¹¶æ±‚å¹³å‡
        elapsed_time_ms = start_event.elapsed_time(end_event)
        avg_time_ms = elapsed_time_ms / TEST_ITERS

        # 5. ç²¾åº¦éªŒè¯ä¸è¯¦ç»†æ•°æ®æ‰“å°
        print("\n" + "="*30)
        print("ğŸ“Š [ç²¾åº¦ç»Ÿè®¡ Precision Stats]")
        
        # è®¡ç®—è¯¯å·®
        diff = torch.abs(output - golden)
        max_err = torch.max(diff).item()
        mean_err = torch.mean(diff).item()
        
        print(f"æœ€å¤§ç»å¯¹è¯¯å·® (Max Error): {max_err:.6f}")
        print(f"å¹³å‡ç»å¯¹è¯¯å·® (Mean Error): {mean_err:.6f}")
        
        # æ‰“å°å¤šç»„æ•°æ®å¯¹æ¯” (å¤´éƒ¨ã€ä¸­é—´ã€å°¾éƒ¨)
        def print_sample(name, tensor):
            t = tensor.cpu().numpy()
            print(f"{name} æ ·ä¾‹:")
            print(f"  å‰5ä¸ª: {t[0, :5]}")
            print(f"  ä¸­é—´5ä¸ª: {t[BATCH_SIZE//2, N_DIM//2 : N_DIM//2+5]}")
            print(f"  æœ«å°¾5ä¸ª: {t[-1, -5:]}")

        print_sample("NPU è¾“å‡º", output)
        print_sample("Reference è¾“å‡º", golden)
    
        print("\nâ±ï¸ [æ€§èƒ½ç»Ÿè®¡ Performance Stats]")
        print(f"å¹³å‡è¿è¡Œè€—æ—¶: {avg_time_ms:.4f} ms")
        print(f"å•æ¬¡ç†è®ºååé‡ (å‡è®¾): { (BATCH_SIZE * K_DIM * N_DIM * 2) / (avg_time_ms / 1000) / 1e12 :.2f} TFLOPS")
        print("="*30 + "\n")

        # 5. æ·±åº¦ç²¾åº¦éªŒè¯ (è·³è¿‡ç©ºå­è½½æ³¢)
        print("\n" + "="*30)
        print("ğŸ” [æœ‰æ•ˆæ•°æ®æå–éªŒè¯]")

        # æ‰¾åˆ° Reference ä¸­ç¬¬ä¸€ä¸ªéé›¶å…ƒç´ çš„ç´¢å¼•
        # golden çš„å½¢çŠ¶æ˜¯ [1192, 512]
        nonzero_indices = torch.nonzero(golden)

        if nonzero_indices.shape[0] == 0:
            print("âŒ è­¦å‘Š: å‚è€ƒè¾“å‡º (Golden) å…¨æ˜¯ 0ï¼è¯·æ£€æŸ¥è¾“å…¥æ•°æ®æˆ–ç”Ÿæˆé€»è¾‘ã€‚")
        else:
            # å–ç¬¬ä¸€ä¸ªéé›¶ç‚¹çš„ä½ç½®
            first_idx = nonzero_indices[0]
            r, c = first_idx[0].item(), first_idx[1].item()
            
            # ç¡®ä¿åˆ‡ç‰‡ä¸è¶Šç•Œ
            c_start = max(0, c)
            c_end = min(N_DIM, c + 8)

            print(f"æ£€æµ‹åˆ°æœ‰æ•ˆæ•°æ®èµ·å§‹ä½ç½®: Batch={r}, Column={c}")
            print(f"NPU å¯¹åº”ç‰‡æ®µ: {output[r, c_start:c_end].cpu().numpy()}")
            print(f"Ref å¯¹åº”ç‰‡æ®µ: {golden[r, c_start:c_end].cpu().numpy()}")

            # è®¡ç®—è¯¥ç‰‡æ®µçš„è¯¯å·®
            segment_err = torch.abs(output[r, c_start:c_end] - golden[r, c_start:c_end]).max()
            print(f"è¯¥ç‰‡æ®µæœ€å¤§è¯¯å·®: {segment_err.item():.6f}")

        # 6. å…¨å±€ç»Ÿè®¡
        max_err = torch.max(torch.abs(output - golden)).item()
        print(f"\nå…¨å±€æœ€å¤§ç»å¯¹è¯¯å·®: {max_err:.6f}")
        
        # ç»Ÿè®¡éé›¶å æ¯”ï¼Œç¡®è®¤æ•°æ®åˆ†å¸ƒ
        npu_nz_count = torch.count_nonzero(output).item()
        total_elements = output.numel()
        print(f"æ•°æ®æ´»è·ƒåº¦ (éé›¶å æ¯”): {npu_nz_count / total_elements * 100:.2f}%")

        # æ–­è¨€ç²¾åº¦æ˜¯å¦åˆæ ¼
        self.assertRtolEqual(output, golden, prec=1e-3)

if __name__ == "__main__":
    if not torch.npu.is_available():
        print("é”™è¯¯: NPU ç¯å¢ƒä¸å¯ç”¨")
        sys.exit(1)
    run_tests()